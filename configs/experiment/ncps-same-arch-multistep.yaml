# @package _global_
defaults:
  - override /model: gcn
  - override /trainer: n-cps


trainer:
  train:
    total_epochs: 350
  num_models: 2
  init:
    optimizer:
      - _target_: torch.optim.AdamW
        _partial_: true
        lr: 0.05
        weight_decay: 0.01
    scheduler:
      - _target_: torch.optim.lr_scheduler.MultiStepLR
        _partial_: true
        milestones: [30, 100]
        gamma: 0.1

logger:
  name: ${model.name}_${trainer.method}_n${trainer.num_models}_lr${trainer.init.optimizer.0.lr}_wd${trainer.init.optimizer.0.weight_decay}_multistep
