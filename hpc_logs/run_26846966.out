Loaded dependency [python3/3.10.12]: sqlite3/3.42.0
Loaded module: python3/3.10.12

Loading python3/3.10.12
  Loading requirement: sqlite3/3.42.0
Loaded module: gcc/13.3.0-binutils-2.42
wandb: Currently logged in as: tanja-pock (tanja-pock-technical-university-of-munich) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run dsnuoaxb
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in ./logs/wandb/run-20251110_135204-dsnuoaxb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gcn_seed=0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tanja-pock-technical-university-of-munich/semi-supervised-ensembles
wandb: üöÄ View run at https://wandb.ai/tanja-pock-technical-university-of-munich/semi-supervised-ensembles/runs/dsnuoaxb
/zhome/3d/c/222266/ComputationalTools/AmazonReview/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 0
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 250
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.001
      weight_decay: 0.005
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 1
      gamma: 0.975
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: ${model.name}_seed=${seed}
  job_type: test
  entity: tanja-pock-technical-university-of-munich
  project_name: semi-supervised-ensembles
  disable: false
  local_dir: ./logs
model:
  name: gcn
  init:
    _target_: models.GCN
    _partial_: false
    num_node_features: 11
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
  0%|          | 0/250 [00:00<?, ?it/s]/zhome/3d/c/222266/.lsbatch/1762779084.26846966.shell: line 20: 155567 Killed                  python3 src/run.py

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 26846966: <run> in cluster <dcc> Exited

Job <run> was submitted from host <n-62-27-22> by user <s252263> in cluster <dcc> at Mon Nov 10 13:51:24 2025
Job was executed on host(s) <n-62-11-16>, in queue <gpuv100>, as user <s252263> in cluster <dcc> at Mon Nov 10 13:51:25 2025
</zhome/3d/c/222266> was used as the home directory.
</zhome/3d/c/222266/gnn_intro> was used as the working directory.
Started at Mon Nov 10 13:51:25 2025
Terminated at Mon Nov 10 13:53:03 2025
Results reported at Mon Nov 10 13:53:03 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q gpuv100
#BSUB -J run GCN
#BSUB -n 8
#BSUB -R "rusage[mem=32GB]"
#BSUB -R "select[gpu]"
#BSUB -R "span[hosts=1]"
#BSUB -gpu "num=1"
#BSUB -W 08:00
#BSUB -o hpc_logs/gcn_%J.out
#BSUB -e hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
source "/zhome/3d/c/222266/ComputationalTools/AmazonReview/.venv/bin/activate"

python3 src/run.py 
 

------------------------------------------------------------

TERM_MEMLIMIT: job killed after reaching LSF memory usage limit.
Exited with exit code 137.

Resource usage summary:

    CPU time :                                   12.00 sec.
    Max Memory :                                 1024 MB
    Average Memory :                             653.67 MB
    Total Requested Memory :                     1024.00 MB
    Delta Memory :                               0.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                22
    Run time :                                   98 sec.
    Turnaround time :                            99 sec.

The output (if any) is above this job summary.

