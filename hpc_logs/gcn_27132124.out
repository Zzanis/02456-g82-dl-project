dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 0
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 250
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.0003
      weight_decay: 0.0001
    scheduler:
      _target_: torch.optim.lr_scheduler.CosineAnnealingLR
      _partial_: true
      T_max: 250
      eta_min: 1.0e-05
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: ${model.name}_seed=${seed}
  job_type: test
  entity: adopetr-danmarks-tekniske-universitet-dtu
  project_name: semi-supervised-ensembles
  disable: false
  local_dir: ./logs
model:
  name: schnet_like
  init:
    _target_: models.SchNetLikeModel
    _partial_: false
    num_node_features: 11
    hidden_channels: 512
    dropout: 0.15
num_models: 2
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mschnet_like_seed=0[0m at: [34mhttps://wandb.ai/adopetr-danmarks-tekniske-universitet-dtu/semi-supervised-ensembles/runs/lplk3cp4[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27132124: <sage_conv> in cluster <dcc> Done

Job <sage_conv> was submitted from host <n-62-27-18> by user <s194778> in cluster <dcc> at Sat Nov 22 10:32:52 2025
Job was executed on host(s) <8*n-62-20-14>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Sat Nov 22 10:32:54 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Sat Nov 22 10:32:54 2025
Terminated at Sat Nov 22 10:37:33 2025
Results reported at Sat Nov 22 10:37:33 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
# LSBATCH: User input
#!/bin/bash
#BSUB -q gpuv100
#BSUB -J sage_conv
#BSUB -n 8
#BSUB -R "rusage[mem=32000]"
#BSUB -M 32000
# GPU & placement
#BSUB -R "select[gpu]"
#BSUB -R "span[hosts=1]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 08:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
# module load cuda/12.1
module load gcc
source "/zhome/f9/1/147385/my_venv/bin/activate"

#python3 src/run.py logger.name=sage trainer=semi-supervised-ensemble model=sage 

python3 src/run.py trainer=semi-supervised-ensemble-custom model=schnet_model
#logger.name=gcn_residual_lr0.005_wd0.002 trainer=semi-supervised-ensemble model=gcn_residual trainer.init.optimizer.lr=0.005 trainer.init.optimizer.weight_decay=0.002
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   784.67 sec.
    Max Memory :                                 2371 MB
    Average Memory :                             2188.75 MB
    Total Requested Memory :                     256000.00 MB
    Delta Memory :                               253629.00 MB
    Max Swap :                                   -
    Max Processes :                              14
    Max Threads :                                71
    Run time :                                   279 sec.
    Turnaround time :                            281 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27132124.err> for stderr output of this job.

