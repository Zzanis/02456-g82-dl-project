dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 1
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 400
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    lambda_max: 0
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.005
      weight_decay: 0.01
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 10
      gamma: 0.9
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: A_gcn_lambda_0
  job_type: test
  entity: DL-gr82
  project_name: Meanest Teacher
  disable: false
  local_dir: ./logs
model:
  name: advanced_gcn
  init:
    _target_: models.advanced_GCN
    _partial_: false
    num_node_features: 11
    hidden_channels: 128
    dropout: 0.05
    use_layer_norm: false
    use_residual: true
    use_kaiming_init: false
num_models: 1
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
Supervised batches: 105, Unsupervised batches: 942
tensor(0., device='cuda:0')
tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mA_gcn_lambda_0[0m at: [34mhttps://wandb.ai/DL-gr82/Meanest%20Teacher/runs/7zu3fhuq[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27270150[1]: <lambda[1-7]> in cluster <dcc> Done

Job <lambda[1-7]> was submitted from host <n-62-27-20> by user <s194778> in cluster <dcc> at Wed Dec  3 21:38:29 2025
Job was executed on host(s) <4*n-62-20-6>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Wed Dec  3 21:38:59 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Wed Dec  3 21:38:59 2025
Terminated at Wed Dec  3 21:51:58 2025
Results reported at Wed Dec  3 21:51:58 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# BSUB job array: submit with `bsub < scripts/run_lr_sweep.sh`
# Adjust learning rates in LR_VALUES below.
#BSUB -J lambda[1-7]
#BSUB -q gpuv100
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=10000]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 04:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
# module load cuda/12.1  # uncomment if needed
source "/zhome/f9/1/147385/my_venv/bin/activate"

# BATCH_VALUES=(50 150 200)
# BATCH=${BATCH_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     dataset.init.batch_size_train=$BATCH \
#     logger.name="A_gcn_MT_GraphNorm_batch${BATCH}" \
#     "$@"


# LR_VALUES=(0.0001 0.0005 0.001 0.005 0.01)
# LR=${LR_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     trainer.init.optimizer.lr=$LR \
#     logger.name="A_gcn_MT_GraphNorm_lr${LR}" \
#     "$@"

#  # Define the list of lambda_unsup values to sweep
# # # LAMBDA_VALUES=(0.001 0.005 0.01 0.05 0.1)
LAMBDA_VALUES=(0 0.01 0.03 0.05 0.08 1 1.5)
LAMBDA=${LAMBDA_VALUES[$((LSB_JOBINDEX-1))]}

# for LAMBDA in "${LAMBDA_VALUES[@]}"; do
#     echo "Running experiment with lambda_unsup = $LAMBDA"

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1472.32 sec.
    Max Memory :                                 2646 MB
    Average Memory :                             2321.00 MB
    Total Requested Memory :                     40000.00 MB
    Delta Memory :                               37354.00 MB
    Max Swap :                                   -
    Max Processes :                              14
    Max Threads :                                62
    Run time :                                   780 sec.
    Turnaround time :                            809 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27270150.err> for stderr output of this job.

dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 1
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 400
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    lambda_max: 0.01
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.005
      weight_decay: 0.01
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 10
      gamma: 0.9
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: A_gcn_lambda_0.01
  job_type: test
  entity: DL-gr82
  project_name: Meanest Teacher
  disable: false
  local_dir: ./logs
model:
  name: advanced_gcn
  init:
    _target_: models.advanced_GCN
    _partial_: false
    num_node_features: 11
    hidden_channels: 128
    dropout: 0.05
    use_layer_norm: false
    use_residual: true
    use_kaiming_init: false
num_models: 1
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
Supervised batches: 105, Unsupervised batches: 942
tensor(0., device='cuda:0')
tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)
dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 1
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 400
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    lambda_max: 0.03
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.005
      weight_decay: 0.01
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 10
      gamma: 0.9
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: A_gcn_lambda_0.03
  job_type: test
  entity: DL-gr82
  project_name: Meanest Teacher
  disable: false
  local_dir: ./logs
model:
  name: advanced_gcn
  init:
    _target_: models.advanced_GCN
    _partial_: false
    num_node_features: 11
    hidden_channels: 128
    dropout: 0.05
    use_layer_norm: false
    use_residual: true
    use_kaiming_init: false
num_models: 1
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
Supervised batches: 105, Unsupervised batches: 942
tensor(0., device='cuda:0')
tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)
dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 1
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 400
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    lambda_max: 0.05
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.005
      weight_decay: 0.01
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 10
      gamma: 0.9
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: A_gcn_lambda_0.05
  job_type: test
  entity: DL-gr82
  project_name: Meanest Teacher
  disable: false
  local_dir: ./logs
model:
  name: advanced_gcn
  init:
    _target_: models.advanced_GCN
    _partial_: false
    num_node_features: 11
    hidden_channels: 128
    dropout: 0.05
    use_layer_norm: false
    use_residual: true
    use_kaiming_init: false
num_models: 1
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
Supervised batches: 105, Unsupervised batches: 942
tensor(0., device='cuda:0')
tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)
dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 1
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 400
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    lambda_max: 0.08
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.005
      weight_decay: 0.01
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 10
      gamma: 0.9
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: A_gcn_lambda_0.08
  job_type: test
  entity: DL-gr82
  project_name: Meanest Teacher
  disable: false
  local_dir: ./logs
model:
  name: advanced_gcn
  init:
    _target_: models.advanced_GCN
    _partial_: false
    num_node_features: 11
    hidden_channels: 128
    dropout: 0.05
    use_layer_norm: false
    use_residual: true
    use_kaiming_init: false
num_models: 1
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
Supervised batches: 105, Unsupervised batches: 942
tensor(0., device='cuda:0')
tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mA_gcn_lambda_0.01[0m at: [34mhttps://wandb.ai/DL-gr82/Meanest%20Teacher/runs/arl9l3t2[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27270150[2]: <lambda[1-7]> in cluster <dcc> Done

Job <lambda[1-7]> was submitted from host <n-62-27-20> by user <s194778> in cluster <dcc> at Wed Dec  3 21:38:29 2025
Job was executed on host(s) <4*n-62-20-6>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Wed Dec  3 21:51:59 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Wed Dec  3 21:51:59 2025
Terminated at Wed Dec  3 22:20:44 2025
Results reported at Wed Dec  3 22:20:44 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# BSUB job array: submit with `bsub < scripts/run_lr_sweep.sh`
# Adjust learning rates in LR_VALUES below.
#BSUB -J lambda[1-7]
#BSUB -q gpuv100
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=10000]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 04:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
# module load cuda/12.1  # uncomment if needed
source "/zhome/f9/1/147385/my_venv/bin/activate"

# BATCH_VALUES=(50 150 200)
# BATCH=${BATCH_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     dataset.init.batch_size_train=$BATCH \
#     logger.name="A_gcn_MT_GraphNorm_batch${BATCH}" \
#     "$@"


# LR_VALUES=(0.0001 0.0005 0.001 0.005 0.01)
# LR=${LR_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     trainer.init.optimizer.lr=$LR \
#     logger.name="A_gcn_MT_GraphNorm_lr${LR}" \
#     "$@"

#  # Define the list of lambda_unsup values to sweep
# # # LAMBDA_VALUES=(0.001 0.005 0.01 0.05 0.1)
LAMBDA_VALUES=(0 0.01 0.03 0.05 0.08 1 1.5)
LAMBDA=${LAMBDA_VALUES[$((LSB_JOBINDEX-1))]}

# for LAMBDA in "${LAMBDA_VALUES[@]}"; do
#     echo "Running experiment with lambda_unsup = $LAMBDA"

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3442.01 sec.
    Max Memory :                                 4952 MB
    Average Memory :                             3790.00 MB
    Total Requested Memory :                     40000.00 MB
    Delta Memory :                               35048.00 MB
    Max Swap :                                   -
    Max Processes :                              18
    Max Threads :                                80
    Run time :                                   1726 sec.
    Turnaround time :                            2535 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27270150.err> for stderr output of this job.

[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mA_gcn_lambda_0.03[0m at: [34mhttps://wandb.ai/DL-gr82/Meanest%20Teacher/runs/uf8z7pbz[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27270150[3]: <lambda[1-7]> in cluster <dcc> Done

Job <lambda[1-7]> was submitted from host <n-62-27-20> by user <s194778> in cluster <dcc> at Wed Dec  3 21:38:29 2025
Job was executed on host(s) <4*n-62-11-13>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Wed Dec  3 21:52:23 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Wed Dec  3 21:52:23 2025
Terminated at Wed Dec  3 22:25:41 2025
Results reported at Wed Dec  3 22:25:41 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# BSUB job array: submit with `bsub < scripts/run_lr_sweep.sh`
# Adjust learning rates in LR_VALUES below.
#BSUB -J lambda[1-7]
#BSUB -q gpuv100
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=10000]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 04:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
# module load cuda/12.1  # uncomment if needed
source "/zhome/f9/1/147385/my_venv/bin/activate"

# BATCH_VALUES=(50 150 200)
# BATCH=${BATCH_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     dataset.init.batch_size_train=$BATCH \
#     logger.name="A_gcn_MT_GraphNorm_batch${BATCH}" \
#     "$@"


# LR_VALUES=(0.0001 0.0005 0.001 0.005 0.01)
# LR=${LR_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     trainer.init.optimizer.lr=$LR \
#     logger.name="A_gcn_MT_GraphNorm_lr${LR}" \
#     "$@"

#  # Define the list of lambda_unsup values to sweep
# # # LAMBDA_VALUES=(0.001 0.005 0.01 0.05 0.1)
LAMBDA_VALUES=(0 0.01 0.03 0.05 0.08 1 1.5)
LAMBDA=${LAMBDA_VALUES[$((LSB_JOBINDEX-1))]}

# for LAMBDA in "${LAMBDA_VALUES[@]}"; do
#     echo "Running experiment with lambda_unsup = $LAMBDA"

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4069.21 sec.
    Max Memory :                                 5007 MB
    Average Memory :                             4644.19 MB
    Total Requested Memory :                     40000.00 MB
    Delta Memory :                               34993.00 MB
    Max Swap :                                   -
    Max Processes :                              18
    Max Threads :                                80
    Run time :                                   1998 sec.
    Turnaround time :                            2832 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27270150.err> for stderr output of this job.

[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mA_gcn_lambda_0.05[0m at: [34mhttps://wandb.ai/DL-gr82/Meanest%20Teacher/runs/gw1f3hvp[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27270150[4]: <lambda[1-7]> in cluster <dcc> Done

Job <lambda[1-7]> was submitted from host <n-62-27-20> by user <s194778> in cluster <dcc> at Wed Dec  3 21:38:29 2025
Job was executed on host(s) <4*n-62-20-5>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Wed Dec  3 21:57:25 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Wed Dec  3 21:57:25 2025
Terminated at Wed Dec  3 22:26:18 2025
Results reported at Wed Dec  3 22:26:18 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# BSUB job array: submit with `bsub < scripts/run_lr_sweep.sh`
# Adjust learning rates in LR_VALUES below.
#BSUB -J lambda[1-7]
#BSUB -q gpuv100
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=10000]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 04:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
# module load cuda/12.1  # uncomment if needed
source "/zhome/f9/1/147385/my_venv/bin/activate"

# BATCH_VALUES=(50 150 200)
# BATCH=${BATCH_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     dataset.init.batch_size_train=$BATCH \
#     logger.name="A_gcn_MT_GraphNorm_batch${BATCH}" \
#     "$@"


# LR_VALUES=(0.0001 0.0005 0.001 0.005 0.01)
# LR=${LR_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     trainer.init.optimizer.lr=$LR \
#     logger.name="A_gcn_MT_GraphNorm_lr${LR}" \
#     "$@"

#  # Define the list of lambda_unsup values to sweep
# # # LAMBDA_VALUES=(0.001 0.005 0.01 0.05 0.1)
LAMBDA_VALUES=(0 0.01 0.03 0.05 0.08 1 1.5)
LAMBDA=${LAMBDA_VALUES[$((LSB_JOBINDEX-1))]}

# for LAMBDA in "${LAMBDA_VALUES[@]}"; do
#     echo "Running experiment with lambda_unsup = $LAMBDA"

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3445.08 sec.
    Max Memory :                                 4958 MB
    Average Memory :                             4553.14 MB
    Total Requested Memory :                     40000.00 MB
    Delta Memory :                               35042.00 MB
    Max Swap :                                   -
    Max Processes :                              18
    Max Threads :                                80
    Run time :                                   1733 sec.
    Turnaround time :                            2869 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27270150.err> for stderr output of this job.

dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 1
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 400
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    lambda_max: 1
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.005
      weight_decay: 0.01
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 10
      gamma: 0.9
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: A_gcn_lambda_1
  job_type: test
  entity: DL-gr82
  project_name: Meanest Teacher
  disable: false
  local_dir: ./logs
model:
  name: advanced_gcn
  init:
    _target_: models.advanced_GCN
    _partial_: false
    num_node_features: 11
    hidden_channels: 128
    dropout: 0.05
    use_layer_norm: false
    use_residual: true
    use_kaiming_init: false
num_models: 1
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
Supervised batches: 105, Unsupervised batches: 942
tensor(0., device='cuda:0')
tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mA_gcn_lambda_0.08[0m at: [34mhttps://wandb.ai/DL-gr82/Meanest%20Teacher/runs/6uz9era6[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27270150[5]: <lambda[1-7]> in cluster <dcc> Done

Job <lambda[1-7]> was submitted from host <n-62-27-20> by user <s194778> in cluster <dcc> at Wed Dec  3 21:38:29 2025
Job was executed on host(s) <4*n-62-20-2>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Wed Dec  3 21:59:04 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Wed Dec  3 21:59:04 2025
Terminated at Wed Dec  3 22:26:54 2025
Results reported at Wed Dec  3 22:26:54 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# BSUB job array: submit with `bsub < scripts/run_lr_sweep.sh`
# Adjust learning rates in LR_VALUES below.
#BSUB -J lambda[1-7]
#BSUB -q gpuv100
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=10000]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 04:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
# module load cuda/12.1  # uncomment if needed
source "/zhome/f9/1/147385/my_venv/bin/activate"

# BATCH_VALUES=(50 150 200)
# BATCH=${BATCH_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     dataset.init.batch_size_train=$BATCH \
#     logger.name="A_gcn_MT_GraphNorm_batch${BATCH}" \
#     "$@"


# LR_VALUES=(0.0001 0.0005 0.001 0.005 0.01)
# LR=${LR_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     trainer.init.optimizer.lr=$LR \
#     logger.name="A_gcn_MT_GraphNorm_lr${LR}" \
#     "$@"

#  # Define the list of lambda_unsup values to sweep
# # # LAMBDA_VALUES=(0.001 0.005 0.01 0.05 0.1)
LAMBDA_VALUES=(0 0.01 0.03 0.05 0.08 1 1.5)
LAMBDA=${LAMBDA_VALUES[$((LSB_JOBINDEX-1))]}

# for LAMBDA in "${LAMBDA_VALUES[@]}"; do
#     echo "Running experiment with lambda_unsup = $LAMBDA"

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3326.40 sec.
    Max Memory :                                 4962 MB
    Average Memory :                             4542.77 MB
    Total Requested Memory :                     40000.00 MB
    Delta Memory :                               35038.00 MB
    Max Swap :                                   -
    Max Processes :                              18
    Max Threads :                                80
    Run time :                                   1671 sec.
    Turnaround time :                            2905 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27270150.err> for stderr output of this job.

dataset:
  dataset_name: qm9
  num_outputs: 1
  init:
    _target_: qm9.QM9DataModule
    data_augmentation: false
    name: qm9
    target: 2
    data_dir: ./data
    batch_size_train: 100
    batch_size_inference: 2048
    num_workers: 4
    splits:
    - 0.72
    - 0.08
    - 0.1
    - 0.1
    seed: 1
    subset_size: null
trainer:
  method: semi-supervised-ensemble
  train:
    total_epochs: 400
    validation_interval: 10
  init:
    _target_: trainer.SemiSupervisedEnsemble
    lambda_max: 1.5
    supervised_criterion:
      _target_: torch.nn.MSELoss
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.005
      weight_decay: 0.01
    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      step_size: 10
      gamma: 0.9
logger:
  _target_: logger.WandBLogger
  group: ${dataset.dataset_name}
  name: A_gcn_lambda_1.5
  job_type: test
  entity: DL-gr82
  project_name: Meanest Teacher
  disable: false
  local_dir: ./logs
model:
  name: advanced_gcn
  init:
    _target_: models.advanced_GCN
    _partial_: false
    num_node_features: 11
    hidden_channels: 128
    dropout: 0.05
    use_layer_norm: false
    use_residual: true
    use_kaiming_init: false
num_models: 1
seed: 0
device: unset
force_deterministic: false
debug_mode: false
result_dir: ./results
compile_model: false

QM9 dataset loaded with 10466 labeled, 94198 unlabeled, 13083 validation, and 13084 test samples.
Batch sizes: labeled=100, unlabeled=100
Supervised batches: 105, Unsupervised batches: 942
tensor(0., device='cuda:0')
tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mA_gcn_lambda_1.5[0m at: [34mhttps://wandb.ai/DL-gr82/Meanest%20Teacher/runs/1ug4j0uc[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27270150[7]: <lambda[1-7]> in cluster <dcc> Done

Job <lambda[1-7]> was submitted from host <n-62-27-20> by user <s194778> in cluster <dcc> at Wed Dec  3 21:38:29 2025
Job was executed on host(s) <4*n-62-20-2>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Wed Dec  3 22:26:55 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Wed Dec  3 22:26:55 2025
Terminated at Wed Dec  3 22:54:26 2025
Results reported at Wed Dec  3 22:54:26 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# BSUB job array: submit with `bsub < scripts/run_lr_sweep.sh`
# Adjust learning rates in LR_VALUES below.
#BSUB -J lambda[1-7]
#BSUB -q gpuv100
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=10000]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 04:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
# module load cuda/12.1  # uncomment if needed
source "/zhome/f9/1/147385/my_venv/bin/activate"

# BATCH_VALUES=(50 150 200)
# BATCH=${BATCH_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     dataset.init.batch_size_train=$BATCH \
#     logger.name="A_gcn_MT_GraphNorm_batch${BATCH}" \
#     "$@"


# LR_VALUES=(0.0001 0.0005 0.001 0.005 0.01)
# LR=${LR_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     trainer.init.optimizer.lr=$LR \
#     logger.name="A_gcn_MT_GraphNorm_lr${LR}" \
#     "$@"

#  # Define the list of lambda_unsup values to sweep
# # # LAMBDA_VALUES=(0.001 0.005 0.01 0.05 0.1)
LAMBDA_VALUES=(0 0.01 0.03 0.05 0.08 1 1.5)
LAMBDA=${LAMBDA_VALUES[$((LSB_JOBINDEX-1))]}

# for LAMBDA in "${LAMBDA_VALUES[@]}"; do
#     echo "Running experiment with lambda_unsup = $LAMBDA"

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3326.88 sec.
    Max Memory :                                 5030 MB
    Average Memory :                             4747.23 MB
    Total Requested Memory :                     40000.00 MB
    Delta Memory :                               34970.00 MB
    Max Swap :                                   -
    Max Processes :                              18
    Max Threads :                                80
    Run time :                                   1652 sec.
    Turnaround time :                            4557 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27270150.err> for stderr output of this job.

[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mA_gcn_lambda_1[0m at: [34mhttps://wandb.ai/DL-gr82/Meanest%20Teacher/runs/q55c75ti[0m

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27270150[6]: <lambda[1-7]> in cluster <dcc> Done

Job <lambda[1-7]> was submitted from host <n-62-27-20> by user <s194778> in cluster <dcc> at Wed Dec  3 21:38:29 2025
Job was executed on host(s) <4*n-62-20-5>, in queue <gpuv100>, as user <s194778> in cluster <dcc> at Wed Dec  3 22:26:18 2025
</zhome/f9/1/147385> was used as the home directory.
</zhome/f9/1/147385/02456-g82-dl-project> was used as the working directory.
Started at Wed Dec  3 22:26:18 2025
Terminated at Wed Dec  3 22:55:11 2025
Results reported at Wed Dec  3 22:55:11 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# BSUB job array: submit with `bsub < scripts/run_lr_sweep.sh`
# Adjust learning rates in LR_VALUES below.
#BSUB -J lambda[1-7]
#BSUB -q gpuv100
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=10000]"
#BSUB -gpu "num=1"
# Time & logs
#BSUB -W 04:00
#BSUB -o /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.out
#BSUB -e /zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_%J.err

set -euo pipefail

module purge
module load python3/3.10.12
module load gcc
# module load cuda/12.1  # uncomment if needed
source "/zhome/f9/1/147385/my_venv/bin/activate"

# BATCH_VALUES=(50 150 200)
# BATCH=${BATCH_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     dataset.init.batch_size_train=$BATCH \
#     logger.name="A_gcn_MT_GraphNorm_batch${BATCH}" \
#     "$@"


# LR_VALUES=(0.0001 0.0005 0.001 0.005 0.01)
# LR=${LR_VALUES[$((LSB_JOBINDEX-1))]}

# python3 src/run.py \
#     trainer=advanced_gcn_trainer \
#     model=advanced_gcn \
#     trainer.init.optimizer.lr=$LR \
#     logger.name="A_gcn_MT_GraphNorm_lr${LR}" \
#     "$@"

#  # Define the list of lambda_unsup values to sweep
# # # LAMBDA_VALUES=(0.001 0.005 0.01 0.05 0.1)
LAMBDA_VALUES=(0 0.01 0.03 0.05 0.08 1 1.5)
LAMBDA=${LAMBDA_VALUES[$((LSB_JOBINDEX-1))]}

# for LAMBDA in "${LAMBDA_VALUES[@]}"; do
#     echo "Running experiment with lambda_unsup = $LAMBDA"

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3462.00 sec.
    Max Memory :                                 4935 MB
    Average Memory :                             4630.23 MB
    Total Requested Memory :                     40000.00 MB
    Delta Memory :                               35065.00 MB
    Max Swap :                                   -
    Max Processes :                              18
    Max Threads :                                80
    Run time :                                   1733 sec.
    Turnaround time :                            4602 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/f9/1/147385/02456-g82-dl-project/hpc_logs/gcn_27270150.err> for stderr output of this job.

